use anyhow::{Context, Result};
use redb::{Database, TableDefinition, WriteTransaction, ReadableTable, ReadableDatabase, TableHandle};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use colored::Colorize;
use std::fs::{self, File};
use std::path::{Path};
use std::sync::Arc;
use std::time::{SystemTime, UNIX_EPOCH};
use std::io::Cursor;

// Cache Configuration
pub const CACHE_DIR: &str = ".veghcache";
const CACHE_DB_FILE: &str = "cache.redb";
const JSON_CACHE_FILE: &str = "index.json";

// Redb Tables - Single Table Schema
const TABLE_DATA_V3: TableDefinition<&str, &[u8]> = TableDefinition::new("data_v3");
const TABLE_INODES_V3: TableDefinition<u64, &str> = TableDefinition::new("inodes_v3");

// Legacy Tables for Migration
const TABLE_DATA_V3_A: TableDefinition<&str, &[u8]> = TableDefinition::new("data_v3_A");
const TABLE_DATA_V3_B: TableDefinition<&str, &[u8]> = TableDefinition::new("data_v3_B");
const TABLE_INODES_V3_A: TableDefinition<u64, &str> = TableDefinition::new("inodes_v3_A");
const TABLE_INODES_V3_B: TableDefinition<u64, &str> = TableDefinition::new("inodes_v3_B");

const TABLE_META: TableDefinition<&str, &str> = TableDefinition::new("meta");

// Cache Entry Structure
#[derive(Serialize, Deserialize, Debug, Clone, PartialEq)]
pub struct FileCacheEntry {
    pub size: u64,
    pub modified: u64,
    
    #[serde(default)]
    pub ctime_sec: i64,
    #[serde(default)]
    pub ctime_nsec: u32,
    #[serde(default)]
    pub device_id: u64,
    #[serde(default)]
    pub inode: u64,
    
    #[serde(default)]
    pub last_seen: u64,

    pub hash: Option<[u8; 32]>,
    
    // [FV3] Compressed Chunks
    #[serde(default)]
    pub chunks_compressed: Option<Vec<u8>>,
    
    #[serde(default)]
    pub sparse_hash: Option<[u8; 32]>,
}

impl FileCacheEntry {
    pub fn set_chunks(&mut self, chunks: Vec<[u8; 32]>) -> Result<()> {
        let flat: Vec<u8> = chunks.iter().flat_map(|c| c.iter()).copied().collect();
        let compressed = zstd::stream::encode_all(Cursor::new(flat), 3)?; 
        self.chunks_compressed = Some(compressed);
        Ok(())
    }

    pub fn get_chunks(&self) -> Result<Option<Vec<[u8; 32]>>> {
        if let Some(compressed) = &self.chunks_compressed {
            let decompressed = zstd::stream::decode_all(Cursor::new(compressed))?;
            if decompressed.len() % 32 != 0 {
                return Ok(None);
            }
            let count = decompressed.len() / 32;
            let mut chunks = Vec::with_capacity(count);
            for i in 0..count {
                let slice = &decompressed[i*32..(i+1)*32];
                chunks.push(slice.try_into()?);
            }
            Ok(Some(chunks))
        } else {
            Ok(None)
        }
    }
}

// FV3 Manifest Structures
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct ManifestEntry {
    pub path: String,
    pub hash: String,
    pub size: u64,
    pub modified: u64,
    #[serde(default)]
    pub mode: u32,
    #[serde(default)]
    pub chunks: Option<Vec<String>>,
}

#[derive(Serialize, Deserialize, Debug, Clone, Default)]
pub struct SnapshotManifest {
    pub entries: Vec<ManifestEntry>,
}

#[derive(Serialize, Deserialize, Debug, Default)]
struct LegacyVeghCache {
    pub last_snapshot: i64,
    pub files: HashMap<String, LegacyFileCacheEntry>,
}

#[derive(Serialize, Deserialize, Debug, Clone)]
struct LegacyFileCacheEntry {
    pub size: u64,
    pub modified: u64,
    pub inode: u64,
    pub hash: Option<String>,
    pub chunks: Option<Vec<String>>,
    pub sparse_hash: Option<String>,
}

// Reader Handle
#[derive(Clone)]
pub struct CacheReader {
    db: Arc<Database>,
}

impl CacheReader {
    pub fn get(&self, path: &str) -> Result<Option<FileCacheEntry>> {
        let txn = self.db.begin_read()?;
        if let Ok(table) = txn.open_table(TABLE_DATA_V3) {
            if let Some(v) = table.get(path)? {
                let entry: FileCacheEntry = bincode::deserialize(&v.value().to_vec())?;
                return Ok(Some(entry));
            }
        }
        Ok(None)
    }

    pub fn get_path_by_inode(&self, inode: u64) -> Result<Option<String>> {
        if inode == 0 { return Ok(None); }
        let txn = self.db.begin_read()?;
        if let Ok(table) = txn.open_table(TABLE_INODES_V3) {
            if let Some(v) = table.get(inode)? {
                return Ok(Some(v.value().to_string()));
            }
        }
        Ok(None)
    }
}

pub struct CacheDB {
    db: Arc<Database>,
    txn: Option<WriteTransaction>,
}

impl CacheDB {
    pub fn open(source: &Path) -> Result<Self> {
        let cache_dir = source.join(CACHE_DIR);
        if !cache_dir.exists() {
            fs::create_dir(&cache_dir).context("Failed to create cache dir")?;
            let gitignore_path = cache_dir.join(".gitignore");
            if !gitignore_path.exists() {
                 let content = "# Generated by Vegh\n*\n";
                 let _ = fs::write(gitignore_path, content);
            }
        }

        let db_path = cache_dir.join(CACHE_DB_FILE);
        let db = Database::create(&db_path)?;
        let db = Arc::new(db);

        let mut cache_db = Self {
            db: db.clone(),
            txn: Some(db.begin_write()?),
        };

        // Check legacy JSON and migrate if needed
        let json_path = cache_dir.join(JSON_CACHE_FILE);
        if json_path.exists() {
             cache_db.migrate_legacy_json(&json_path)?;
        }
        
        // Check for migration from A/B slots
        cache_db.migrate_v3_slots()?;

        Ok(cache_db)
    }
    
    pub fn reader(&self) -> CacheReader {
        CacheReader {
            db: self.db.clone(),
        }
    }
    
    fn migrate_v3_slots(&mut self) -> Result<()> {
        let tables: Vec<String> = {
            let r_txn = self.db.begin_read()?;
            r_txn.list_tables()?.map(|t| t.name().to_string()).collect()
        };
        
        let txn = self.txn.as_mut().unwrap();
        let mut migrated = false;
        
        // Migrate A
        if tables.contains(&"data_v3_A".to_string()) {
             println!("{} Migrating cache slot A...", "ðŸ“¦".cyan());
             {
                 let mut dest = txn.open_table(TABLE_DATA_V3)?;
                 let source = txn.open_table(TABLE_DATA_V3_A)?;
                 let mut count = 0;
                 for res in source.iter()? {
                     let (k, v) = res?;
                     dest.insert(k.value(), v.value())?;
                     count += 1;
                 }
                 if count > 0 { migrated = true; }
             }
             {
                 let mut dest = txn.open_table(TABLE_INODES_V3)?;
                 let source = txn.open_table(TABLE_INODES_V3_A)?;
                 for res in source.iter()? {
                     let (k, v) = res?;
                     dest.insert(k.value(), v.value())?;
                 }
             }
             txn.delete_table(TABLE_DATA_V3_A)?;
             txn.delete_table(TABLE_INODES_V3_A)?;
        }

        // Migrate B
        if tables.contains(&"data_v3_B".to_string()) {
             println!("{} Migrating cache slot B...", "ðŸ“¦".cyan());
             {
                 let mut dest = txn.open_table(TABLE_DATA_V3)?;
                 let source = txn.open_table(TABLE_DATA_V3_B)?;
                 let mut count = 0;
                 for res in source.iter()? {
                     let (k, v) = res?;
                     dest.insert(k.value(), v.value())?;
                     count += 1;
                 }
                 if count > 0 { migrated = true; }
             }
             {
                 let mut dest = txn.open_table(TABLE_INODES_V3)?;
                 let source = txn.open_table(TABLE_INODES_V3_B)?;
                 for res in source.iter()? {
                     let (k, v) = res?;
                     dest.insert(k.value(), v.value())?;
                 }
             }
             txn.delete_table(TABLE_DATA_V3_B)?;
             txn.delete_table(TABLE_INODES_V3_B)?;
        }
        
        if migrated {
            if tables.contains(&"meta".to_string()) {
                let _ = txn.delete_table(TABLE_META);
            }
            println!("{} Migration complete.", "âœ…".green());
        }
        
        Ok(())
    }

    fn migrate_legacy_json(&mut self, path: &Path) -> Result<()> {
        println!("{} Migrating JSON cache to Embedded DB...", "ðŸ“¦".cyan());
        if let Ok(file) = File::open(path) {
            if let Ok(cache) = serde_json::from_reader::<_, LegacyVeghCache>(file) {
                 let now = SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs();
                 let txn = self.txn.as_mut().unwrap();
                 
                 let mut data = txn.open_table(TABLE_DATA_V3)?;
                 let mut inodes = txn.open_table(TABLE_INODES_V3)?;
                 
                 for (k, v) in cache.files {
                     let hash_bytes = v.hash.and_then(|h| hex::decode(h).ok()).and_then(|v| v.try_into().ok());
                     let sparse_bytes = v.sparse_hash.and_then(|h| hex::decode(h).ok()).and_then(|v| v.try_into().ok());
                     
                     let mut new_entry = FileCacheEntry {
                         size: v.size,
                         modified: v.modified,
                         inode: v.inode,
                         ctime_sec: 0,
                         ctime_nsec: 0,
                         device_id: 0,
                         last_seen: now,
                         hash: hash_bytes,
                         chunks_compressed: None,
                         sparse_hash: sparse_bytes,
                     };
                     
                     if let Some(chunks) = v.chunks {
                         let bin_chunks: Vec<[u8; 32]> = chunks.into_iter()
                             .filter_map(|h| hex::decode(h).ok().and_then(|v| v.try_into().ok()))
                             .collect();
                         let _ = new_entry.set_chunks(bin_chunks);
                     }
                     
                     let bytes = bincode::serialize(&new_entry)?;
                     data.insert(k.as_str(), bytes.as_slice())?;
                     if new_entry.inode > 0 {
                         inodes.insert(new_entry.inode, k.as_str())?;
                     }
                 }
            }
        }
        let _ = fs::remove_file(path);
        Ok(())
    }

    pub fn insert(&mut self, path: &str, entry: &FileCacheEntry) -> Result<()> {
        let txn = self.txn.as_mut().unwrap();
        let bytes = bincode::serialize(entry)?;

        let mut data = txn.open_table(TABLE_DATA_V3)?;
        data.insert(path, bytes.as_slice())?;
        
        if entry.inode > 0 {
            let mut inodes = txn.open_table(TABLE_INODES_V3)?;
            inodes.insert(entry.inode, path)?;
        }
        Ok(())
    }
    
    pub fn commit_batch(&mut self) -> Result<()> {
        if let Some(txn) = self.txn.take() {
            txn.commit()?;
            self.txn = Some(self.db.begin_write()?);
        }
        Ok(())
    }

    pub fn garbage_collect(&mut self, retention_seconds: u64) -> Result<u64> {
        let now = SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs();
        let mut keys_to_remove: Vec<(String, u64)> = Vec::new(); 
        
        {
            let txn = self.txn.as_ref().unwrap();
            if let Ok(table) = txn.open_table(TABLE_DATA_V3) {
                 for res in table.iter()? {
                     let (k, v) = res?;
                     if let Ok(entry) = bincode::deserialize::<FileCacheEntry>(&v.value().to_vec()) {
                         if now.saturating_sub(entry.last_seen) >= retention_seconds {
                             keys_to_remove.push((k.value().to_string(), entry.inode));
                         }
                     }
                 }
            }
        }
        
        let count = keys_to_remove.len() as u64;
        if count > 0 {
             let txn = self.txn.as_mut().unwrap();
             let mut data = txn.open_table(TABLE_DATA_V3)?;
             let mut inodes = txn.open_table(TABLE_INODES_V3)?;
             
             for (path, inode) in keys_to_remove {
                 data.remove(path.as_str())?;
                 if inode > 0 {
                     inodes.remove(inode)?;
                 }
             }
        }

        Ok(count)
    }

    pub fn commit(mut self) -> Result<()> {
        if let Some(txn) = self.txn.take() {
            txn.commit()?;
        }
        Ok(())
    }

    pub fn sync_partial(self) -> Result<()> {
        println!("{} Saving cache (Partial)...", "ðŸ’¾".blue());
        self.commit()?;
        println!("{} Cache saved.", "âœ…".green());
        Ok(())
    }
}
